{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lorenz-63 model\n",
    "In this Notebook we'll demonstrate how to construct multivariate models. To this end, we shall consider the Lorenz-63 model, defined as \n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\mathrm{d}X^1_t = -S (X^1_t - X^2_t) \\mathrm{d}t + \\mathrm{d}W^1_t, \\\\\n",
    "        \\mathrm{d}X^2_t = (RX^1_t - X^2_t - X^1_t \\cdot X^2_t) \\mathrm{d}t + \\mathrm{d}W^2_t, \\\\\n",
    "        \\mathrm{d}X^3_t = (X^1_t \\cdot X^2_t - BX^3_t) \\mathrm{d}t + \\mathrm{d}W^3_t,\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "where $\\{W^i_t\\}$ are 1-dimensional Wienere processes and $(S, R, B) \\in \\mathbb{R}^3$. We assume that we collect discrete observations of the above system via the following set of equations\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        Y^1_t = k_o X^1_t + V^1_t, \\\\\n",
    "        Y^2_t = k_o X^3_t + V^2_t,\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "where $k_o > 0$, and $V^i_t \\sim \\mathcal{N}(0, \\sigma^2)$ where $\\sigma^2 = 0.1$. We proceed in the same manner as the Stochastic volatility example and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\Desktop\\pyfilter\n"
     ]
    }
   ],
   "source": [
    "% cd ..\n",
    "from pyfilter.timeseries import StateSpaceModel, EulerMaruyma, LinearGaussianObservations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the inital distributions as\n",
    "\\begin{equation}\n",
    "X^1_0 \\sim \\mathcal{N}(-5.9162, 10), \\quad X^2_0 \\sim \\mathcal{N}(-5.52332, 10), \\quad X^3_0 \\sim \\mathcal{N}(24.5723, 10),\n",
    "\\end{equation}\n",
    "which means that the model equations are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "def finit(s, r, b):\n",
    "    return torch.tensor([-5.91652, -5.52332, 24.5723], device=s.device)\n",
    "\n",
    "\n",
    "def ginit(s, r, b):\n",
    "    return math.sqrt(10) * torch.ones(3, device=s.device)\n",
    "\n",
    "\n",
    "def f(x, s, r, b):\n",
    "    x1 = -s * (x[0] - x[1])\n",
    "    x2 = r * x[0] - x[1] - x[0] * x[2]\n",
    "    x3 = x[0] * x[1] - b * x[2]\n",
    "\n",
    "    return x1, x2, x3\n",
    "\n",
    "\n",
    "def g(x, s, r, b):\n",
    "    return torch.ones(3, device=x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we'll simulate data, we do this by setting $(S, R, B) \\triangleq (10, 28, 8/3)$ and run the below script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29fe609c3c8>,\n",
       " <matplotlib.lines.Line2D at 0x29fe609cf98>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Independent, Normal, Uniform, MultivariateNormal\n",
    "\n",
    "\n",
    "dt = torch.tensor(1e-2)\n",
    "hidden = EulerMaruyma((finit, ginit), (f, g), (10., 28., 8/3), ndim=3, dt=dt)\n",
    "\n",
    "mat = torch.tensor([\n",
    "    [0.8, 0., 0.],\n",
    "    [0., 0., 0.8]\n",
    "])\n",
    "\n",
    "simmodel = LinearGaussianObservations(hidden, mat, torch.ones(2) / math.sqrt(10))\n",
    "\n",
    "x, y = simmodel.sample(2000)\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(16, 9))\n",
    "\n",
    "ax[1].plot(x.numpy())\n",
    "ax[0].plot(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the upper plot is the observble data, and the lower is the hidden processes. Next, we shall perform the inference. We use the NESS to target the parameters. However, since the model has purely additive noise (rather than multiplicative) in the observational process, we may use the `Unscented Kalman Filter` to target the latent process. Furthermore, we use the same priors as in the original paper describing the NESS algorithm. Such that we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NESS: 100%|████████████████████████████████████████████████████████████████████████| 2000/2000 [04:15<00:00,  7.68it/s]\n",
      "NESS:  71%|██████████████████████████████████████████████████▊                     | 1411/2000 [03:00<01:10,  8.35it/s]"
     ]
    }
   ],
   "source": [
    "from pyfilter.algorithms import NESS\n",
    "from pyfilter.filters import APF\n",
    "from pyfilter.resampling import multinomial\n",
    "\n",
    "hidden = EulerMaruyma((finit, ginit), (f, g), (Uniform(5, 40), Uniform(10, 100), Uniform(1, 20)), ndim=3, dt=dt)\n",
    "ssm = LinearGaussianObservations(hidden, mat, torch.ones(2) / math.sqrt(10))\n",
    "\n",
    "algs = list()\n",
    "for i in range(2):\n",
    "    filt_ = APF(ssm.copy(), 200, resampling=multinomial)\n",
    "    alg = NESS(filt_, 2000)\n",
    "    \n",
    "    alg.initialize().to_('cuda:0').fit(y)\n",
    "    algs.append(alg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we plot the posterior distributions together with the true parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfilter.utils import normalize\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(16, 9))\n",
    "\n",
    "for r, alg in enumerate(algs):\n",
    "    \n",
    "    print('Run {:d}'.format(1 + r))\n",
    "\n",
    "    for i, (p, ptrue) in enumerate(zip(alg.filter.ssm.hidden.theta, simmodel.hidden.theta)):\n",
    "        weights = normalize(alg._w_rec)\n",
    "        xrange, xvals = p.get_plottable(weights=weights)\n",
    "        \n",
    "        ax[i].plot(xrange, xvals)\n",
    "        ax[i].axvline(ptrue.values, color='r')\n",
    "        \n",
    "        print('\\t Posterior mean and std: {:.2f}, {:.2f}'.format(p.values.mean(), p.values.std()))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stochastic volatility\n",
    "This Notebook demos how to construct a stochastic volatility model and fit it to data. We will use a version of the precision model of G. Chacko and L. M. Viceira. `Dynamic consumption and portfolio choice with stochastic volatility in incomplete markets` given by, where we've replaced the mean reverting process with a random walk,\n",
    "\\begin{cases}\n",
    "Y_t = \\mu + e^{-V_t} W_t, \\\\\n",
    "dV_t = \\kappa (\\gamma - V_{t}) + \\sigma dU_t, \\\\\n",
    "\\end{cases}\n",
    "where $\\mu, \\beta, \\gamma \\in \\mathbb{R}$, and $\\kappa, \\sigma \\in \\mathbb{R}_+$.$\\{U_t\\}$ is a Brownian motion, wheras $\\{W_t\\}$ is assumed to follow a Student's t distribution with $\\nu$ degrees of freedom. \n",
    "\n",
    "We begin with importing the necessary libraries for defining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfilter.timeseries import StateSpaceModel, models as m, AffineObservations, Parameter, DistributionBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the governing dynamics, and set the default tensor to be CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "def go(vol, level):\n",
    "    return level\n",
    "\n",
    "\n",
    "def fo(vol, level):\n",
    "    return vol.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall define our model. In order to do so, we must specify priors for the different parameters. Given their support, we assume that\n",
    "\\begin{cases}\n",
    "    \\mu, \\gamma \\sim \\mathcal{N}(0, 1), \\\\\n",
    "    \\kappa \\sim \\mathcal{E}(10), \\\\\n",
    "    \\nu \\sim \\mathcal{E}(0.1), \\\\\n",
    "    \\sigma \\sim \\mathcal{E}(5). \\\\\n",
    "\\end{cases}\n",
    "To do this, we need to import the necessary distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Exponential, Normal, StudentT, Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model in terms of code and get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logvol = m.OrnsteinUhlenbeck(Exponential(10.), Normal(0., 1.), Exponential(5.), ndim=1, dt=1.)\n",
    "\n",
    "student_t = DistributionBuilder(StudentT, df=Parameter(Exponential(0.1)))\n",
    "obs = AffineObservations((go, fo), (Normal(0., 1.),), student_t)\n",
    "\n",
    "stockmodel = StateSpaceModel(logvol, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that defines the model. Next, we need a dataset to train on. We're just going to pick Apple Inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "There are duplicates in the index: [(Timestamp('2020-11-20 00:00:00'), 'QQQ')]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7376db2cb087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0miex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIEXCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://cloud.iexapis.com/stable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_historical_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"QQQ\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2000-01-01\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"changePercent\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda3\\lib\\site-packages\\pygecko\\api\\iexcloud.py\u001b[0m in \u001b[0;36mget_historical_range\u001b[1;34m(self, symbols, from_, to, only_close)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_historical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_close\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda3\\lib\\site-packages\\pygecko\\api\\iexcloud.py\u001b[0m in \u001b[0;36mget_historical\u001b[1;34m(self, symbol, range_, only_close)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_historical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mM1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_close\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch_historical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_close\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0monly_close\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda3\\lib\\site-packages\\pygecko\\api\\iexcloud.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mduplicates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"There are duplicates in the index: {list(res.index[duplicates])}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: There are duplicates in the index: [(Timestamp('2020-11-20 00:00:00'), 'QQQ')]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygecko.api import IEXCloud\n",
    "\n",
    "iex = IEXCloud(\"https://cloud.iexapis.com/stable\")\n",
    "\n",
    "hist = iex.get_historical_range(\"QQQ\", from_=\"2000-01-01\")\n",
    "y = np.log(1 + hist[\"changePercent\"].iloc[1:] / 100) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the data to get an idea of its volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "y.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit the model to the given data, we need an algorithm. We shall use the SMC2 by Nicolas Chopin et al. Furthermore, since we are using particle filters, we need to decide on which proposal to use - should we go with the Bootstrap or something more advanced? For this example, we will use the bootstrap as the problem's so low dimensional. Importing the relevant classes, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfilter.inference.sequential import SMC2\n",
    "from pyfilter.filters import APF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = 5\n",
    "training = torch.from_numpy(y.values[:-predictions]).float()\n",
    "\n",
    "algs = list()\n",
    "for i in range(2):\n",
    "    filt = APF(stockmodel.copy(), 100)\n",
    "    alg = SMC2(filt, 600, n_steps=2)\n",
    "\n",
    "    state = alg.fit(training)\n",
    "    \n",
    "    algs.append((state, alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the posterior distributions. The first corresponds to the standard deviation of the latent process. The last three ones correspond to the mean, exposure to variance and dof of the Student's t distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyfilter.utils import normalize\n",
    "from arviz import plot_posterior\n",
    "\n",
    "fig, ax = plt.subplots(len(alg.filter.ssm.trainable_parameters), figsize=(16, 9))\n",
    "\n",
    "syms = '\\\\kappa, \\\\gamma, \\\\sigma, \\\\mu, \\\\nu'.split(',')\n",
    "colors = [\"lightblue\", \"orange\"]\n",
    "\n",
    "for r, (state, alg) in enumerate(algs):    \n",
    "    w = normalize(state.w)\n",
    "    \n",
    "    for i, param in enumerate(alg.filter.ssm.trainable_parameters):    \n",
    "        plot_posterior(param.cpu().numpy(), ax=ax[i], hdi_prob=\"hide\", point_estimate=None, color=colors[r])\n",
    "        \n",
    "        ax[i].set_title(f\"${syms[i]}$\")\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a crude estimate of the underlying volatility. I say crude here because we uniformly weight the trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "trunc = y.loc['2010-01-01':].iloc[:-predictions]\n",
    "\n",
    "indx = y.index[:training.shape[0]]\n",
    "for state, alg in algs:\n",
    "    asdf = pd.DataFrame(state.filter_state.filter_means.exp().cpu().numpy(), index=indx).mean(1)    \n",
    "    asdf.loc[trunc.index].plot(alpha=0.75, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the volatility spike caused by the COVID breakout.\n",
    "\n",
    "A plot over the ESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "for state, alg in algs:\n",
    "    ax.plot(alg.logged_ess.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's, finally, get an estimate of the return distribution 5 steps (days) into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "for i, (state, alg) in enumerate(algs):    \n",
    "    _, yt = alg.predict(predictions, state, aggregate=False)  \n",
    "            \n",
    "    mask = yt.abs() > 100\n",
    "    yt[mask] = 100 * yt[mask].sign()\n",
    "    \n",
    "    final = pd.Series(yt.sum(0).view(-1).cpu().numpy()).sort_values()\n",
    "    final = final[int(0.05 * final.size):int(0.95 * final.size):50]\n",
    "\n",
    "    final.plot.kde(ax=ax, label=f\"Prediction {i + 1:d}\")\n",
    "\n",
    "y.iloc[:-predictions].rolling(predictions).sum().rename(f'Historic rolling {predictions:d} day return').plot.kde(ax=ax)\n",
    "ax.set_xlim(-20, 20)\n",
    "ax.plot(y.iloc[-predictions:].sum(), 0, 'ro', label='Outcome')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

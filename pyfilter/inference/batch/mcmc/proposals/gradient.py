import torch
from pyro.distributions import Normal
from torch.autograd import grad

from .random_walk import RandomWalk


# Merge...
def _model_likelihood(
        time: torch.Tensor,
        x: torch.Tensor,
        y: torch.Tensor,
        parameters, # We pass parameters as they need to be included, they're used implicitly
        state,
        model,
        context
        ) -> torch.Tensor:
    # Create states
    x_tm1 = state.propagate_from(values=x[:-1], time_increment=time[:-1])
    x_t = state.propagate_from(values=x[1:], time_increment=time[1:])

    # Create densities
    hidden_dens = model.hidden.build_density(x_tm1)
    obs_dens = model.build_density(x_t)

    y = y.reshape(y.shape[:1] + torch.Size([1 for _ in hidden_dens.batch_shape[1:]]) + obs_dens.event_shape)        

    return (
        model.hidden.initial_distribution.log_prob(x[0]).mean(0) +
        context.eval_priors(constrained=False) +
        (hidden_dens.log_prob(x_t.value) + obs_dens.log_prob(y)).sum(0).mean(0)
    )


class GradientBasedProposal(RandomWalk):
    r"""
    Implements a proposal kernel that utilizes the gradient of the total log likelihood, which we define as
        .. math::
            S(\theta) \coloneqq \log{p_\theta(y_{1:t})} + \log{p(\theta)}.

    The candidate kernel :math:`\theta^*` is then generated by
        .. math::
            \theta^* \sim \mathcal{N} \left (\theta + \epsilon \nabla S(\theta), \sqrt{2\epsilon} \right),

    where :math:`\theta` denotes the latest accepted parameter candidate, and :math:`\epsilon` the step size. Note
    that we generate the kernels on the constrained space of the parameters.
    """

    def __init__(self, use_second_order: bool = False, **kwargs):
        """
        Internal initializer for :class:`GradientBasedProposal`.

        Args:
            use_second_order (bool, optional): whether to use seconrd order information when constructing proposal
            kernel. Defaults to False.
        """

        if use_second_order:
            raise NotImplementedError("Currently does not support `use_second_order`!")

        super().__init__(**kwargs)
        self._eps = self._scale ** 2.0 / 2.0
        self._use_second_order = use_second_order

    # TODO: Use functorch...
    def build(self, context, state, filter_, y):
        # Smooth
        smoothed = filter_.smooth(state.filter_state.states)        
        time = torch.stack([s.timeseries_state.time_index for s in state.filter_state.states])

        # As the first state's time value is zero, we use that
        first_state = state.filter_state.states[0].get_timeseries_state()
                
        # Create densities
        parameters = context.parameters.values()
        for parameter in parameters:
            parameter.requires_grad_(True)

        # TODO: Use functorch...
        model = filter_._model_builder(context)     # We recreate model as the gradients haven't been registered
        logl = _model_likelihood(time, smoothed, y, parameters, first_state, model, context)
        gradients = grad(logl, parameters, torch.ones_like(logl), is_grads_batched=True)

        if self._use_second_order:
            raise NotImplementedError("Second order information is currently not implemented!")

        locs = tuple()
        for (name, parameter), g in zip(context.get_parameters(), gradients):
            parameter.detach_()
            unconstrained = (parameter.get_unconstrained() + self._eps * g).view(-1, context.get_shape(name, False).numel())
            
            locs += (unconstrained,)

        loc = torch.cat(locs, dim=-1)        
        return Normal(loc=loc, scale=self._scale).to_event(1)

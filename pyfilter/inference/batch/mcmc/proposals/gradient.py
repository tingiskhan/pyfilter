from torch.distributions import Independent, Normal
import torch
from torch.autograd import grad
from .random_walk import RandomWalk
from .....timeseries import NewState


class GradientBasedProposal(RandomWalk):
    """
    Implements a proposal kernel that utilizes the gradient of the total log likelihood, which we define as
        .. math::
            S(\\theta) \\coloneqq \\log{p_\\theta(y_{1:t})} + \\log{p(\\theta)}.

    The candidate kernel :math:`\\theta^*` is then generated by
        .. math::
            \\theta^* \\sim \\mathcal{N} \\left (\\theta + \\epsilon \\nabla S(\\theta), \\sqrt{2\\epsilon} \\right),

    where :math:`\\theta` denotes the latest accepted parameter candidate, and :math:`\\epsilon` the step size. Note
    that we generate the kernels on the constrained space of the parameters.
    """

    def __init__(self, use_second_order: bool = False, **kwargs):
        """
        Initializes the ``GradientBasedProposal`` class.

        Args:
            use_second_order: Optional parameter specifying whether to use second order information when constructing
                the proposal kernel. In practice this means that we utilize the diagonal of the Hessian.
            kwargs: See base.
        """

        super().__init__(**kwargs)
        self._eps = self._scale ** 2.0 / 2.0
        self._use_second_order = use_second_order

    def build(self, state, filter_, y):
        smoothed = filter_.smooth(state.filter_state.states)

        params = filter_.ssm.concat_parameters(constrained=False)
        params.requires_grad_(True)

        filter_.ssm.update_parameters_from_tensor(params, constrained=False)

        time = torch.stack(tuple(s.x.time_index for s in state.filter_state.states))

        xtm1 = NewState(time[:-1], values=smoothed[:-1])
        xt = NewState(time[1:], values=smoothed[1:])

        y = y.view(y.shape[0], 1, 1, *y.shape[1:])

        hidden_dens = filter_.ssm.hidden.build_density(xtm1)
        obs_dens = filter_.ssm.observable.build_density(xt)

        logl = filter_.ssm.hidden.initial_dist.log_prob(smoothed[0]).mean(-1)
        logl += filter_.ssm.eval_prior_log_prob(constrained=False).squeeze(-1)
        logl = (hidden_dens.log_prob(xt.values) + obs_dens.log_prob(y)).mean(-1).sum(0)

        g = grad(logl, params, torch.ones_like(logl), create_graph=self._use_second_order)[-1]

        step = self._eps * torch.ones_like(params)
        scale = self._scale * torch.ones_like(params)

        if self._use_second_order:
            raise NotImplementedError("Second order information is currently not implemented!")

        loc = params + step * g
        params.detach_()

        # TODO: Better?
        for v in filter_.ssm.parameters():
            v.detach_()

        return Independent(Normal(loc, scale), 1)
